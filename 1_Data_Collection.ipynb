{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A company by the name of Alpha Analytics has hired me to procure a model that will accurately predict the origin of text posts and state what subreddit the text came from. Alpha Analytics has years of Reddit data stored for analytic purposes. When they hired a contractor from Upwork to assist with cleaning the data, the contractor accidentally deleted the classification feature on a large segment of the dataset. At the moment, they have no way of telling the text in the dataframe apart. Fortunately, I have developed a way to assemble similar datasets to represent the Alpha Analytics dilemna. I will test a number of models and explore performance metrics to understand what type of model will be most accurate in predicting the subreddit origin. I am using submissions from the Fallout subreddit and the No Mans Sky subreddit in order to simulate Alpha Analytics situation. Once I can prove the fidelity of the model on these datasets, I will move on to adapting the model to the partially deleted dataset, and restore with significant accuracy, the classification assignments of each document. With this massive dataset restored, Alpha Analytics can further predict user behavior and market needs with confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "With the datasets I've gathered, can I build a model that will accurately predict document classifacation, with the origin of the document being either the Fallout or No Mans Sky subreddits. Is there enough strength in the relationships between word choice in the Fallout subreddit to distinguish it from the No Mans Sky subreddit, and vice versa? Of the selection of models that I have at my disposal, what model will perform best in predicting class assignment for reddit submissions.\n",
    "\n",
    "I believe that with the data gathered and the EDA / data cleaning done thus far, I will be able to accurately predict the origin of a submission, and assign it to either the Fallout subreddit, or the No Mans Sky subreddit with above 0.90% confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0026419162750244"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start=time.time()\n",
    "time.sleep(1)\n",
    "current=time.time()\n",
    "current - start"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.reddit.com/r/Fallout/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = 'submission'\n",
    "subreddit = 'Fallout'\n",
    "day_window = 15\n",
    "i = 2\n",
    "\n",
    "BASE_URL = f\"https://api.pushshift.io/reddit/search/{kind}\" # also known as the \"API endpoint\"\n",
    "stem = f\"{BASE_URL}?subreddit={subreddit}&size=100\" # always pulling max of 500\n",
    "URL = \"{}&after={}d\".format(stem, day_window * i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This function below uses pushshift to interact with the reddit API and scrape the data from the provided subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pushshift(subreddit, kind = 'submission', day_window = 15, n = 160):\n",
    "    \n",
    "    SUBFIELDS = ['title', 'selftext', 'subreddit', 'created_utc', 'author', 'num_comments', 'score', 'is_self']\n",
    "    \n",
    "    # establish base url and stem\n",
    "    # could also use request.getparams\n",
    "    BASE_URL = f\"https://api.pushshift.io/reddit/search/{kind}\" # also known as the \"API endpoint\" \n",
    "    stem = f\"{BASE_URL}?subreddit={subreddit}&size=100\" # always pulling max of 100\n",
    "    \n",
    "    # instantiate empty list for temp storage\n",
    "    posts = []\n",
    "    \n",
    "    # implement for loop with `time.sleep(2)`\n",
    "    for i in range(1, n + 1):\n",
    "        URL = \"{}&after={}d\".format(stem, day_window * i)\n",
    "        #print(\"Querying from: \" + URL)\n",
    "        response = requests.get(URL)\n",
    "        assert response.status_code == 200\n",
    "        mine = response.json()['data']\n",
    "        df = pd.DataFrame.from_dict(mine)\n",
    "        posts.append(df)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # pd.concat storage list\n",
    "    full = pd.concat(posts, sort=False)\n",
    "    \n",
    "    # if submission\n",
    "    if kind == \"submission\":\n",
    "        # select desired columns\n",
    "        full = full[SUBFIELDS]\n",
    "        # drop duplicates\n",
    "        full.drop_duplicates(inplace = True)\n",
    "        # select `is_self` == True\n",
    "        full = full.loc[full['is_self'] == True]\n",
    "\n",
    "    # create `timestamp` column\n",
    "    full['timestamp'] = full[\"created_utc\"].map(dt.date.fromtimestamp)\n",
    "    \n",
    "    print(\"Query Complete!\")    \n",
    "    return full.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Complete!\n"
     ]
    }
   ],
   "source": [
    "falloutresults = query_pushshift(\"Fallout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14621, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falloutresults.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pushshift(subreddit, kind = 'submission', day_window = 7, n = 300):\n",
    "    \n",
    "    SUBFIELDS = ['title', 'selftext', 'subreddit', 'created_utc', 'author', 'num_comments', 'score', 'is_self']\n",
    "    \n",
    "    # establish base url and stem\n",
    "    # could also use request.getparams\n",
    "    BASE_URL = f\"https://api.pushshift.io/reddit/search/{kind}\" # also known as the \"API endpoint\" \n",
    "    stem = f\"{BASE_URL}?subreddit={subreddit}&size=100\" # always pulling max of 100\n",
    "    \n",
    "    # instantiate empty list for temp storage\n",
    "    posts = []\n",
    "    \n",
    "    # implement for loop with `time.sleep(2)`\n",
    "    for i in range(1, n + 1):\n",
    "        URL = \"{}&after={}d\".format(stem, day_window * i)\n",
    "        #print(\"Querying from: \" + URL)\n",
    "        response = requests.get(URL)\n",
    "        assert response.status_code == 200\n",
    "        mine = response.json()['data']\n",
    "        df = pd.DataFrame.from_dict(mine)\n",
    "        posts.append(df)\n",
    "        #print(df.shape)\n",
    "        time.sleep(1.5)\n",
    "    # pd.concat storage list\n",
    "    full = pd.concat(posts, sort=False)\n",
    "    \n",
    "    \n",
    "    # if submission\n",
    "    if kind == \"submission\":\n",
    "        # select desired columns\n",
    "        full = full[SUBFIELDS]\n",
    "        # drop duplicates\n",
    "        full.drop_duplicates(inplace = True)\n",
    "        # select `is_self` == True\n",
    "        full = full.loc[full['is_self'] == True]\n",
    "        \n",
    "    # create `timestamp` column\n",
    "    full['timestamp'] = full[\"created_utc\"].map(dt.date.fromtimestamp)\n",
    "    \n",
    "    print(\"Query Complete!\")    \n",
    "    return full.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = 'submission'\n",
    "subreddit = 'NoMansSkyTheGame'\n",
    "day_window = 15\n",
    "i = 2\n",
    "\n",
    "BASE_URL = f\"https://api.pushshift.io/reddit/search/{kind}\" # also known as the \"API endpoint\"\n",
    "stem = f\"{BASE_URL}?subreddit={subreddit}&size=100\" # always pulling max of 500\n",
    "URL = \"{}&after={}d\".format(stem, day_window * i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.reddit.com/r/NoMansSkyTheGame/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Complete!\n"
     ]
    }
   ],
   "source": [
    "nomansskyresults = query_pushshift(\"NoMansSkyTheGame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14758, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nomansskyresults.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14621, 9)\n",
      "(14758, 9)\n"
     ]
    }
   ],
   "source": [
    "print(falloutresults.shape)\n",
    "print(nomansskyresults.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(falloutresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nomansskyresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ch/Desktop/GA/Projects/project_3'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "falloutresults.to_csv('Datasets/falloutresults.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomansskyresults.to_csv('Datasets/nomansskyresults.csv', index = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Checking to make sure the datasets are the same shape generally speaking for later concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14621, 9), (14758, 9))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falloutresults.shape, nomansskyresults.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
